"use strict";(self.webpackChunkfluid_website_demo=self.webpackChunkfluid_website_demo||[]).push([[2416],{2616:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>u,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var t=a(7624),s=a(2172);const i={sidebar_label:"Cache Runtime Auto Scaling",sidebar_position:2},c="Cache Runtime Auto Scaling",r={id:"operation-guide/cache-runtime-auto-scaling",title:"Cache Runtime Auto Scaling",description:"Fluid can disperse data into Kubernetes compute nodes by creating Dataset objects as a medium for data exchange, which can effectively avoid remote writing and reading of data and improve the efficiency of data usage.",source:"@site/versioned_docs/version-v1.0/operation-guide/cache-runtime-auto-scaling.md",sourceDirName:"operation-guide",slug:"/operation-guide/cache-runtime-auto-scaling",permalink:"/zh/docs/operation-guide/cache-runtime-auto-scaling",draft:!1,unlisted:!1,editUrl:"https://github.com/fluid-cloudnative/fluid-cloudnative.github.io/tree/master/versioned_docs/version-v1.0/operation-guide/cache-runtime-auto-scaling.md",tags:[],version:"v1.0",lastUpdatedBy:"chenqiming",lastUpdatedAt:1714050288,formattedLastUpdatedAt:"2024\u5e744\u670825\u65e5",sidebarPosition:2,frontMatter:{sidebar_label:"Cache Runtime Auto Scaling",sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Runtime Monitoring",permalink:"/zh/docs/operation-guide/runtime-monitoring"},next:{title:"How to Develop",permalink:"/zh/docs/developer-guide/how-to-develop"}},o={},l=[{value:"Prerequisite",id:"prerequisite",level:2},{value:"Steps",id:"steps",level:2},{value:"Summary",id:"summary",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.M)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"cache-runtime-auto-scaling",children:"Cache Runtime Auto Scaling"}),"\n",(0,t.jsx)(n.p,{children:"Fluid can disperse data into Kubernetes compute nodes by creating Dataset objects as a medium for data exchange, which can effectively avoid remote writing and reading of data and improve the efficiency of data usage.\nBut the problem here is the resource estimation and provisioning of the data cache. Since accurate data prediction is more difficult to meet before data production and consumption, using on-demand scaling is more user-friendly.\nThe on-demand scaling technology is similar to page cache, which is transparent to the user, but the acceleration it brings is obvious."}),"\n",(0,t.jsx)(n.p,{children:"Fluid introduces cache elasticity scaling capabilities through a custom HPA mechanism. The condition for elastic scaling is that the elastic expansion of cache space occurs when the amount of existing cache data reaches a certain percentage. For example, if the trigger condition is set to more than 80% and the total cache space is 10G, when the data fills up to 8G of cache space, the scaling will be triggered."}),"\n",(0,t.jsx)(n.p,{children:"This document will show you this feature."}),"\n",(0,t.jsx)(n.h2,{id:"prerequisite",children:"Prerequisite"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"It is recommended to use Kubernetes 1.18 onwards, because before 1.18, HPA is not able to customize the scaling policy, it is hard-coded. After 1.18, users can customize the scale-out and scale-in policies, such as defining the cooling time after a scaling."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Fluid has been installed. If not, please follow the ",(0,t.jsx)(n.a,{href:"/docs/get-started/installation",children:"installation guide"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"steps",children:"Steps"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Install the jq tool to facilitate parsing json, in this case we are using CentOS, and can install jq via yum"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ yum install -y jq\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"2",children:["\n",(0,t.jsx)(n.li,{children:"Download the community repo if needed"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ git clone https://github.com/fluid-cloudnative/community.git\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"3",children:["\n",(0,t.jsx)(n.li,{children:"Deploy or configure Prometheus"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Metrics exposed by the cache engine of AlluxioRuntime are collected here by Prometheus. If there is no Prometheus in your cluster, please follow the ",(0,t.jsx)(n.a,{href:"https://prometheus.io/docs/prometheus/latest/installation/",children:"Installation guide"})," to set up Prometheus correctly in your production environment.:"]}),"\n",(0,t.jsx)(n.p,{children:"If you have Prometheus in your cluster, you can write the following configuration to the Prometheus configuration file:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"scrape_configs:\n  - job_name: 'alluxio runtime'\n    metrics_path: /metrics/prometheus\n    kubernetes_sd_configs:\n      - role: endpoints\n    relabel_configs:\n    - source_labels: [__meta_kubernetes_service_label_monitor]\n      regex: alluxio_runtime_metrics\n      action: keep\n    - source_labels: [__meta_kubernetes_endpoint_port_name]\n      regex: web\n      action: keep\n    - source_labels: [__meta_kubernetes_namespace]\n      target_label: namespace\n      replacement: $1\n      action: replace\n    - source_labels: [__meta_kubernetes_service_label_release]\n      target_label: fluid_runtime\n      replacement: $1\n      action: replace\n    - source_labels: [__meta_kubernetes_endpoint_address_target_name]\n      target_label: pod\n      replacement: $1\n      action: replace\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"4",children:["\n",(0,t.jsx)(n.li,{children:"Verify that the Prometheus installation was successful"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl get ep -n kube-system  prometheus-svc\nNAME             ENDPOINTS        AGE\nprometheus-svc   10.76.0.2:9090   6m49s\n$ kubectl get svc -n kube-system prometheus-svc\nNAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\nprometheus-svc   NodePort   172.16.135.24   <none>        9090:32114/TCP   2m7s\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If you want to visualize monitoring metrics, you can install Grafana to verify monitoring data, as described in ",(0,t.jsx)(n.a,{href:"/docs/operation-guide/runtime-monitoring",children:"this documentation"}),"."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:a(7812).c+"",width:"1890",height:"931"})}),"\n",(0,t.jsxs)(n.ol,{start:"5",children:["\n",(0,t.jsx)(n.li,{children:"Deploy metrics server"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Check if the cluster includes a metrics-server, run ",(0,t.jsx)(n.code,{children:"kubectl top node"})," with correct output for memory and CPU, then the cluster metrics server is correctly configured."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl top node\nNAME                       CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\n192.168.1.204   93m          2%     1455Mi          10%\n192.168.1.205   125m         3%     1925Mi          13%\n192.168.1.206   96m          2%     1689Mi          11%\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Otherwise, you need to install metrics server from ",(0,t.jsx)(n.a,{href:"https://github.com/helm/charts/tree/master/stable/metrics-server",children:"metrics helm chart repo"}),"."]}),"\n",(0,t.jsxs)(n.ol,{start:"6",children:["\n",(0,t.jsx)(n.li,{children:"Deploy custom-metrics-api component"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"In order to scale based on custom metrics, you need to have two components. The first component collects metrics from the application and stores them in the Prometheus time series database. The second component uses the collected metrics to extend the Kubernetes custom metrics API: k8s-prometheus-adapter. The first component is deployed in step 3, and the second component is deployed below."}),"\n",(0,t.jsx)(n.p,{children:"If custom-metrics-api is already configured, add the dataset-related configuration to the adapter's ConfigMap configuration."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: adapter-config\n  namespace: monitoring\ndata:\n  config.yaml: |\n    rules:\n    - seriesQuery: \'{__name__=~"Cluster_(CapacityTotal|CapacityUsed)",fluid_runtime!="",instance!="",job="alluxio runtime",namespace!="",pod!=""}\'\n      seriesFilters:\n      - is: ^Cluster_(CapacityTotal|CapacityUsed)$\n      resources:\n        overrides:\n          namespace:\n            resource: namespace\n          pod:\n            resource: pods\n          fluid_runtime:\n            resource: datasets\n      name:\n        matches: "^(.*)"\n        as: "capacity_used_rate"\n      metricsQuery: ceil(Cluster_CapacityUsed{<<.LabelMatchers>>}*100/(Cluster_CapacityTotal{<<.LabelMatchers>>}))\n'})}),"\n",(0,t.jsx)(n.p,{children:"Otherwise, manually execute the following command:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl create -f integration/custom-metrics-api\n"})}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsx)(n.p,{children:"Note: Because custom-metrics-api connects to the Prometheous access address in the cluster, please replace the Prometheous url with the Prometheous address you actually use."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Check custom metrics:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'$ kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1" | jq\n{\n  "kind": "APIResourceList",\n  "apiVersion": "v1",\n  "groupVersion": "custom.metrics.k8s.io/v1beta1",\n  "resources": [\n    {\n      "name": "pods/capacity_used_rate",\n      "singularName": "",\n      "namespaced": true,\n      "kind": "MetricValueList",\n      "verbs": [\n        "get"\n      ]\n    },\n    {\n      "name": "datasets.data.fluid.io/capacity_used_rate",\n      "singularName": "",\n      "namespaced": true,\n      "kind": "MetricValueList",\n      "verbs": [\n        "get"\n      ]\n    },\n    {\n      "name": "namespaces/capacity_used_rate",\n      "singularName": "",\n      "namespaced": false,\n      "kind": "MetricValueList",\n      "verbs": [\n        "get"\n      ]\n    }\n  ]\n}\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"7",children:["\n",(0,t.jsx)(n.li,{children:"Submit the Dataset used for testing"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'$ cat<<EOF >dataset.yaml\napiVersion: data.fluid.io/v1alpha1\nkind: Dataset\nmetadata:\n  name: spark\nspec:\n  mounts:\n    - mountPoint: https://mirrors.bit.edu.cn/apache/spark/\n      name: spark\n---\napiVersion: data.fluid.io/v1alpha1\nkind: AlluxioRuntime\nmetadata:\n  name: spark\nspec:\n  replicas: 1\n  tieredstore:\n    levels:\n      - mediumtype: MEM\n        path: /dev/shm\n        quota: 1Gi\n        high: "0.99"\n        low: "0.7"\n  properties:\n    alluxio.user.streaming.data.timeout: 300sec\nEOF\n$ kubectl create -f dataset.yaml\ndataset.data.fluid.io/spark created\nalluxioruntime.data.fluid.io/spark created\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"8",children:["\n",(0,t.jsx)(n.li,{children:"Checking whether the Dataset is available, we can see that the total amount of data in the dataset is 2.71GiB, the number of cache nodes provided by Fluid is 1, and the maximum cache capacity available is 1GiB. At this time, the amount of data cannot meet the demand for full data caching."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl get dataset\nNAME    UFS TOTAL SIZE   CACHED   CACHE CAPACITY   CACHED PERCENTAGE   PHASE   AGE\nspark   2.71GiB          0.00B    1.00GiB          0.0%                Bound   7m38s\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"9",children:["\n",(0,t.jsx)(n.li,{children:"When the Dataset is available, see if the monitoring metrics are already available from the custom-metrics-api"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'$ kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/datasets.data.fluid.io/*/capacity_used_rate" | jq\n{\n  "kind": "MetricValueList",\n  "apiVersion": "custom.metrics.k8s.io/v1beta1",\n  "metadata": {\n    "selfLink": "/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/datasets.data.fluid.io/%2A/capacity_used_rate"\n  },\n  "items": [\n    {\n      "describedObject": {\n        "kind": "Dataset",\n        "namespace": "default",\n        "name": "spark",\n        "apiVersion": "data.fluid.io/v1alpha1"\n      },\n      "metricName": "capacity_used_rate",\n      "timestamp": "2021-04-04T07:24:52Z",\n      "value": "0"\n    }\n  ]\n}\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"10",children:["\n",(0,t.jsx)(n.li,{children:"Create HPA"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'$ cat<<EOF > hpa.yaml\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: spark\nspec:\n  scaleTargetRef:\n    apiVersion: data.fluid.io/v1alpha1\n    kind: AlluxioRuntime\n    name: spark\n  minReplicas: 1\n  maxReplicas: 4\n  metrics:\n  - type: Object\n    object:\n      metric:\n        name: capacity_used_rate\n      describedObject:\n        apiVersion: data.fluid.io/v1alpha1\n        kind: Dataset\n        name: spark\n      target:\n        type: Value\n        value: "90"\n  behavior:\n    scaleUp:\n      policies:\n      - type: Pods\n        value: 2\n        periodSeconds: 600\n    scaleDown:\n      selectPolicy: Disabled\nEOF\n'})}),"\n",(0,t.jsx)(n.p,{children:"First of all, let's explain the configuration from the sample, there are two main parts here: one is the rule of scaling, and the other is the sensitivity of scaling."}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Rule: The condition to trigger the scaling behavior is that the cached data amount of Dataset object accounts for 90% of the total cache capacity; the scaling object is AlluxioRuntime, the minimum number of replicas is 1 and the maximum number of replicas is 4; and the objects of Dataset and AlluxioRuntime need to be in the same namespace."}),"\n",(0,t.jsx)(n.li,{children:"Policy: For K8s version 1.18 or higher, you can set the stability time and the ratio of scaling steps for scale-out and scale-in scenarios respectively. In this example, the scale-out period is 10 minutes (periodSeconds), and 2 replicas are added during the scale-out, which of course cannot exceed the maxReplicas limit; after the scale-out is completed, the cooling time (stabilizationWindowSeconds) is 20 minutes; and the scale-in policy can be chosen to close directly."}),"\n"]}),"\n",(0,t.jsxs)(n.ol,{start:"11",children:["\n",(0,t.jsx)(n.li,{children:"Checking the HPA configuration, the current cache space has a data percentage of 0. This is far below the condition that triggers the scale-out."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'$ kubectl get hpa\nNAME    REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nspark   AlluxioRuntime/spark   0/90      1         4         1          33s\n$ kubectl describe hpa\nName:                                                    spark\nNamespace:                                               default\nLabels:                                                  <none>\nAnnotations:                                             <none>\nCreationTimestamp:                                       Wed, 07 Apr 2021 17:36:39 +0800\nReference:                                               AlluxioRuntime/spark\nMetrics:                                                 ( current / target )\n  "capacity_used_rate" on Dataset/spark (target value):  0 / 90\nMin replicas:                                            1\nMax replicas:                                            4\nBehavior:\n  Scale Up:\n    Stabilization Window: 0 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Pods  Value: 2  Period: 600 seconds\n  Scale Down:\n    Select Policy: Disabled\n    Policies:\n      - Type: Percent  Value: 100  Period: 15 seconds\nAlluxioRuntime pods:   1 current / 1 desired\nConditions:\n  Type            Status  Reason               Message\n  ----            ------  ------               -------\n  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation\n  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from Dataset metric capacity_used_rate\n  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range\nEvents:           <none>\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"12",children:["\n",(0,t.jsx)(n.li,{children:"Create DataLoad"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"$ cat<<EOF > dataload.yaml\napiVersion: data.fluid.io/v1alpha1\nkind: DataLoad\nmetadata:\n  name: spark\nspec:\n  dataset:\n    name: spark\n    namespace: default\nEOF\n$ kubectl create -f dataload.yaml\n$ kubectl get dataload\nNAME    DATASET   PHASE       AGE   DURATION\nspark   spark     Executing   15s   Unfinished\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"13",children:["\n",(0,t.jsx)(n.li,{children:"Now you can see that the amount of data cached is close to the caching capacity that Fluid can provide (1GiB) and the scaling condition is triggered"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$  kubectl  get dataset\nNAME    UFS TOTAL SIZE   CACHED       CACHE CAPACITY   CACHED PERCENTAGE   PHASE   AGE\nspark   2.71GiB          1020.92MiB   1.00GiB          36.8%               Bound   5m15s\n"})}),"\n",(0,t.jsx)(n.p,{children:"From the HPA monitor, we can see that the scale-out of Alluxio Runtime has started, and we can see that the scale-out step is 2"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:'$ kubectl get hpa\nNAME    REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nspark   AlluxioRuntime/spark   100/90    1         4         2          4m20s\n$ kubectl describe hpa\nName:                                                    spark\nNamespace:                                               default\nLabels:                                                  <none>\nAnnotations:                                             <none>\nCreationTimestamp:                                       Wed, 07 Apr 2021 17:56:31 +0800\nReference:                                               AlluxioRuntime/spark\nMetrics:                                                 ( current / target )\n  "capacity_used_rate" on Dataset/spark (target value):  100 / 90\nMin replicas:                                            1\nMax replicas:                                            4\nBehavior:\n  Scale Up:\n    Stabilization Window: 0 seconds\n    Select Policy: Max\n    Policies:\n      - Type: Pods  Value: 2  Period: 600 seconds\n  Scale Down:\n    Select Policy: Disabled\n    Policies:\n      - Type: Percent  Value: 100  Period: 15 seconds\nAlluxioRuntime pods:   2 current / 3 desired\nConditions:\n  Type            Status  Reason              Message\n  ----            ------  ------              -------\n  AbleToScale     True    SucceededRescale    the HPA controller was able to update the target scale to 3\n  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from Dataset metric capacity_used_rate\n  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range\nEvents:\n  Type     Reason                        Age                    From                       Message\n  ----     ------                        ----                   ----                       -------\n  Normal   SuccessfulRescale             21s                    horizontal-pod-autoscaler  New size: 2; reason: Dataset metric capacity_used_rate above target\n  Normal   SuccessfulRescale             6s                     horizontal-pod-autoscaler  New size: 3; reason: Dataset metric capacity_used_rate above target\n'})}),"\n",(0,t.jsxs)(n.ol,{start:"14",children:["\n",(0,t.jsx)(n.li,{children:"After waiting for a while, the cache space of the dataset is increased from 1GiB to 3GiB, and the data cache is almost completed"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl  get dataset\nNAME    UFS TOTAL SIZE   CACHED    CACHE CAPACITY   CACHED PERCENTAGE   PHASE   AGE\nspark   2.71GiB          2.59GiB   3.00GiB          95.6%               Bound   12m\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"15",children:["\n",(0,t.jsx)(n.li,{children:"If we observe the status of the HPA, we can see that the number of replicas of the runtime corresponding to the Dataset is 3, and the ratio of capacity_used_rate to the cache space already used is 85%, which will not trigger the cache scale-out."}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl get hpa\nNAME    REFERENCE              TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nspark   AlluxioRuntime/spark   85/90     1         4         3          11m\n"})}),"\n",(0,t.jsxs)(n.ol,{start:"16",children:["\n",(0,t.jsx)(n.li,{children:"Clean the environment"}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-shell",children:"$ kubectl delete hpa spark\n$ kubectl delete dataset spark\n"})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Fluid provides a combination of Prometheous, Kubernetes HPA and Custom Metrics capabilities to trigger automatic elastic scaling based on the proportion of cache space occupied, enabling on-demand use of cache capacity. This helps users to use distributed caching more flexibly to improve data access acceleration, and we will provide the ability to scale up and down at regular intervals to provide stronger determinism for scaling."})]})}function u(e={}){const{wrapper:n}={...(0,s.M)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},7812:(e,n,a)=>{a.d(n,{c:()=>t});const t=a.p+"assets/images/dataset_auto_scaling-83d70eb14b20cff550ce8e545fe59444.png"},2172:(e,n,a)=>{a.d(n,{I:()=>r,M:()=>c});var t=a(1504);const s={},i=t.createContext(s);function c(e){const n=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);