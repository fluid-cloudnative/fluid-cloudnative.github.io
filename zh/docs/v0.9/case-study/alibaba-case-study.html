<!doctype html>
<html lang="zh" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-v0.9 docs-doc-page docs-doc-id-case-study/alibaba-case-study" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Alibaba&#x27;s Case Study | Fluid</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://fluid-cloudnative.github.io/zh/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://fluid-cloudnative.github.io/zh/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://fluid-cloudnative.github.io/zh/docs/v0.9/case-study/alibaba-case-study"><meta data-rh="true" property="og:locale" content="zh"><meta data-rh="true" property="og:locale:alternate" content="en"><meta data-rh="true" name="docusaurus_locale" content="zh"><meta data-rh="true" name="docsearch:language" content="zh"><meta data-rh="true" name="docusaurus_version" content="v0.9"><meta data-rh="true" name="docusaurus_tag" content="docs-default-v0.9"><meta data-rh="true" name="docsearch:version" content="v0.9"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-v0.9"><meta data-rh="true" property="og:title" content="Alibaba&#x27;s Case Study | Fluid"><meta data-rh="true" name="description" content="This article discusses the significant role of Fluid with JindoCache in the large-scale model training within Alibaba Group."><meta data-rh="true" property="og:description" content="This article discusses the significant role of Fluid with JindoCache in the large-scale model training within Alibaba Group."><link data-rh="true" rel="icon" href="/zh/img/fluid-icon-color.png"><link data-rh="true" rel="canonical" href="https://fluid-cloudnative.github.io/zh/docs/v0.9/case-study/alibaba-case-study"><link data-rh="true" rel="alternate" href="https://fluid-cloudnative.github.io/docs/v0.9/case-study/alibaba-case-study" hreflang="en"><link data-rh="true" rel="alternate" href="https://fluid-cloudnative.github.io/zh/docs/v0.9/case-study/alibaba-case-study" hreflang="zh"><link data-rh="true" rel="alternate" href="https://fluid-cloudnative.github.io/docs/v0.9/case-study/alibaba-case-study" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://LFPGLLH528-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/zh/blog/rss.xml" title="Fluid RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh/blog/atom.xml" title="Fluid Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Fluid" href="/zh/opensearch.xml"><link rel="stylesheet" href="/zh/assets/css/styles.8e3a1f9d.css">
<script src="/zh/assets/js/runtime~main.5c6a7f1d.js" defer="defer"></script>
<script src="/zh/assets/js/main.9c023251.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh/"><div class="navbar__logo"><img src="/zh/img/fluid-horizontal-color.png" alt="" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/zh/img/fluid-horizontal-white.png" alt="" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate"></b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh/docs/v0.9">文档</a><a class="navbar__item navbar__link" href="/zh/blog">博客</a><a class="navbar__item navbar__link" sidebarid="communitySidebar" href="/zh/community/meeting_schedule">社区</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a aria-current="page" class="navbar__link active" aria-haspopup="true" aria-expanded="false" role="button" href="/zh/docs/v0.9">v0.9</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/zh/docs/next/case-study/alibaba-case-study">Latest</a></li><li><a class="dropdown__link" href="/zh/docs/case-study/alibaba-case-study">v1.0</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/zh/docs/v0.9/case-study/alibaba-case-study">v0.9</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>简体中文</a><ul class="dropdown__menu"><li><a href="/docs/v0.9/case-study/alibaba-case-study" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/zh/docs/v0.9/case-study/alibaba-case-study" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="zh">简体中文</a></li></ul></div><a href="https://github.com/fluid-cloudnative/fluid" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切换浅色/暗黑模式（当前为浅色模式）" aria-label="切换浅色/暗黑模式（当前为浅色模式）" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="搜索"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">搜索</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到顶部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文档侧边栏" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9">Core Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/get-started/quick-start">Get Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/demos/demo-accelerate-remote-file-accessing-with-fluid">Demos</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/tutorials/dataset-creation/accelerate-data-accessing-posix">Tutorials</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/operation-guide/runtime-monitoring">Operation Guide</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/developer-guide/how-to-develop">Developer Guide</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/zh/docs/v0.9/case-study/alibaba-case-study">Case Study</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/zh/docs/v0.9/case-study/alibaba-case-study">Alibaba&#x27;s Case Study</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh/docs/v0.9/case-study/weibo-case-study">Weibo&#x27;s Case Study</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh/docs/v0.9/case-study/metabit-trading-case-study">Metabit Trading&#x27;s Case Study</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh/docs/v0.9/case-study/haomo-case-study">HAOMO&#x27;s Case Study</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/zh/docs/v0.9/case-study/zuoyebang-case-study">Zuoyebang&#x27;s Case Study</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/troubleshooting-and-faq/troubleshooting">Troubleshooting and FAQ</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/zh/docs/v0.9/release-and-API-doc/release">Release and API Doc</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="theme-doc-version-banner alert alert--warning margin-bottom--md" role="alert"><div>此为 <!-- -->Fluid<!-- --> <b>v0.9</b> 版的文档，现已不再积极维护。</div><div class="margin-top--md">最新的文档请参阅 <b><a href="/zh/docs/case-study/alibaba-case-study">最新版本</a></b> (<!-- -->v1.0<!-- -->)。</div></div><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="页面路径"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主页面" class="breadcrumbs__link" href="/zh/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Case Study</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Alibaba&#x27;s Case Study</span><meta itemprop="position" content="2"></li></ul></nav><span class="theme-doc-version-badge badge badge--secondary">版本：v0.9</span><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本页总览</button></div><div class="theme-doc-markdown markdown"><h1>Alibaba&#x27;s Case Study</h1>
<blockquote>
<p>This article discusses the significant role of Fluid with JindoCache in the large-scale model training within Alibaba Group.</p>
</blockquote>
<p><em>By Wang Tao (Yangli), Chen Qiukai (Qiusuo) and Xu Zhihao (Dongyun)</em></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-background-information">1. Background Information<a href="#1-background-information" class="hash-link" aria-label="1. Background Information的直接链接" title="1. Background Information的直接链接">​</a></h2>
<p>In 2024, new technology trends, such as the large model, artificial intelligence generated content (AIGC), and multi-modal technologies, begin to be integrated with actual business and put into production. These new technology trends not only increase the requirement for computing power, but also bring greater challenges to the underlying infrastructure.</p>
<p>In the computing field, heterogeneous hardware, such as graphics processing units (GPUs) and field programmable gate arrays (FPGAs), adapts to changing needs through short-cycle iterations and evolution. Alibaba Group adopts a variety of scheduling methods, such as centralized scheduling, unified resource pool, and comprehensive elasticity, to meet complex computing needs.</p>
<p>In the storage field, classic microservices applications achieve a balance between performance and efficiency based on the cloud-native architecture. However, for computing-intensive applications with the largest increase in the computing capacity, such as distributed AI training and big data, data locality directly affects the operation efficiency and throughput of computing jobs. The consumption of network I/O operations also indirectly increases the bandwidth costs. In addition, in predictable scenarios, the size of datasets will continue to grow rapidly. Accelerating data access with appropriate data cache affinity technologies will be the key to improving the efficiency of computing jobs while reducing costs.</p>
<p>Datasets in large model training and multimedia scenarios mainly consist of pictures and audio files. <a href="https://www.alibabacloud.com/en/product/object-storage-service" target="_blank" rel="noopener noreferrer">Object Storage Service</a> (OSS) is naturally suitable for managing the data. It is also a storage service for most online computing jobs. For example, data read operations in training scenarios have the following characteristics:</p>
<ol>
<li>
<p>Sequential randomization of datasets causes failures of conventional standalone cache policies.</p>
</li>
<li>
<p>Datasets are read for multiple times during multiple epochs.</p>
</li>
<li>
<p>A dataset may be reused in different jobs.</p>
</li>
</ol>
<p>To sum up, distributed caches and file systems are naturally suitable for accelerating I/O operations in multiple AI platform business domains of Alibaba Group.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-chanllenges">2. Chanllenges<a href="#2-chanllenges" class="hash-link" aria-label="2. Chanllenges的直接链接" title="2. Chanllenges的直接链接">​</a></h2>
<ol>
<li>
<p>The compute-storage separation architecture improves the flexibility of data access and horizontal scaling of computing resources, but leads to a high latency in data access. This is unfriendly to training scenarios with a high requirement for data cache affinity. Machine learning tasks for business teams must frequently access sample datasets and checkpoints in OSS in real time during training. When OSS has limited bandwidth or high load, data access to OSS is one to two orders of magnitude slower than that to local files, and causes high bandwidth costs.</p>
</li>
<li>
<p>The Kubernetes scheduler is unaware of data caches, and access to a data source remains slow after multiple times of access. In practice, the same data is repeatedly accessed by deep learning tasks, including tasks with the same model but different hyperparameters, tasks with fine-tuned models but the same input, and AutoML tasks. Repeated data access by the deep learning tasks results in reusable data caches. However, because the native Kubernetes scheduler is unaware of caches, the result of application scheduling is poor, caches cannot be reused, and performance cannot be improved.</p>
</li>
<li>
<p>OSS has become the bottleneck of concurrent data access, and stability is faced with great challenges. A large number of machine learning tasks concurrently access OSS during simultaneous training. Such concurrent machine learning training causes high I/O load, and OSS is prone to single points of failure (SPOFs). When bandwidth of OSS is limited, all machine learning tasks are affected.</p>
</li>
<li>
<p>Training files are scattered, and metadata access is under high pressure. Training data files of machine learning tasks are usually scattered in different paths, and list operations for reading the files take an extended period of time. The performance of list operations in OSS is suboptimal. When performed at a large scale, these operations are prone to timeouts or failures due to the heavy load on metadata access.</p>
</li>
<li>
<p>I/O stability directly affects business operation. Poor I/O stability leads to unstable business performance or even causes task failures. FUSE-based storage clients are more prone to such problems. If these problems cannot be automatically solved, cluster training tasks may be interrupted. Maintaining I/O stability is one of the keys to ensuring smooth business operation.</p>
</li>
</ol>
<p>Based on analysis of the preceding typical data access patterns, it is found in practice that expensive computing resources such as GPUs cannot be fully utilized due to I/O performance issues. The nature of machine learning training results in scattered data file access and high metadata access load. If metadata and file data can be cached in a fine-grained manner, cache efficiency and disk utilization can be improved, and metadata loss caused by file lookup operations can also be mitigated.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-efficient-cache-scheduling-acceleration-system-for-deep-learning-tasks">3. Efficient Cache Scheduling Acceleration System for Deep Learning Tasks<a href="#3-efficient-cache-scheduling-acceleration-system-for-deep-learning-tasks" class="hash-link" aria-label="3. Efficient Cache Scheduling Acceleration System for Deep Learning Tasks的直接链接" title="3. Efficient Cache Scheduling Acceleration System for Deep Learning Tasks的直接链接">​</a></h2>
<p>To improve efficiency of large-scale machine learning model training for Alibaba Group, better data localization must be achieved for data access during model training. Therefore, the following objectives are established:</p>
<ol>
<li>
<p>ully utilize local data access in computing: Repeated read operations through the network are prevented, and the time taken by I/O operations in the computing pipeline is minimized, thus accelerating training for machine learning models and improving GPU utilization of clusters.</p>
</li>
<li>
<p>Reduce the load on OSS: Applications read some data from local storage, thus reducing the latency in data access and the load on the bandwidth of OSS.</p>
</li>
<li>
<p>Leverage the cache nodes for hot datasets: Tasks are intelligently scheduled onto data cache nodes without user awareness, thus accelerating the operation of common model training programs.</p>
</li>
<li>
<p>Separate metadata caches from data caches: You can separately cache metadata of files and customize cache policies.</p>
</li>
<li>
<p>Read data through POSIX-based APIs: You do not need to use different data access APIs during model development and training, thus reducing the costs of developing machine learning model programs.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="31-components-of-the-project-architecture">3.1 Components of the Project Architecture<a href="#31-components-of-the-project-architecture" class="hash-link" aria-label="3.1 Components of the Project Architecture的直接链接" title="3.1 Components of the Project Architecture的直接链接">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="fluid">Fluid<a href="#fluid" class="hash-link" aria-label="Fluid的直接链接" title="Fluid的直接链接">​</a></h4>
<p><a href="https://github.com/fluid-cloudnative/fluid" target="_blank" rel="noopener noreferrer">Fluid</a> is an open source scalable distributed data orchestration and acceleration system. It enables data access for data-intensive applications such as AI and big data based on the Kubernetes standard without user awareness. It is intended to build an efficient support platform for data-intensive applications in cloud-native environments. Based on data layer abstraction provided by Kubernetes services, Fluid can flexibly and efficiently move, replicate, evict, transform, and manage data between storage sources such as HDFS, OSS, and Ceph and upper-layer cloud-native computing applications of Kubernetes. Specific data operations are performed without user awareness. You do not need to worry about the efficiency of accessing remote data, the convenience of managing data sources, or how to help Kubernetes make O&amp;M and scheduling decisions. You can directly access abstracted data from Kubernetes-native persistent volumes (PVs) and persistent volume claims (PVCs). Remaining tasks and underlying details are all handled by Fluid.</p>
<p><img decoding="async" loading="lazy" alt="Fluid" src="/zh/assets/images/ali-fluid-0eac4aa5089416b15556997fc205f339.png" width="1830" height="908" class="img_ev3q"></p>
<p>Fluid supports multiple runtimes, including JindoRuntime, AlluxioRuntime, JuiceFSRuntime, and GooseFSRuntime. JindoRuntime has outstanding capabilities, performance, and stability, and is applied in many scenarios. <a href="https://github.com/aliyun/alibabacloud-jindodata/blob/master/docs/user/6.x/6.2.0/jindo_fluid/jindo_fluid_overview.md" target="_blank" rel="noopener noreferrer">JindoRuntime</a> is a distributed cache runtime of Fluid. It is built on JindoCache, a distributed cache acceleration engine.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="jindocache">JindoCache<a href="#jindocache" class="hash-link" aria-label="JindoCache的直接链接" title="JindoCache的直接链接">​</a></h4>
<p>JindoCache, formerly known as JindoFSx, is a cloud-native data lake acceleration service provided by the data lake management team of Alibaba Cloud. JindoCache supports acceleration features such as data caching and metadata caching. JindoCache can use different CacheSets for different file paths to provide different read/write policies to meet the requirements for access acceleration in different scenarios of data lakes.</p>
<p>JindoCache is applicable to the following scenarios:</p>
<ul>
<li>
<p>Online analytical processing (OLAP) (Presto queries), to improve query performance and reduce query time.</p>
</li>
<li>
<p>DataServing (HBase), to significantly reduce the P99 latency and request costs.</p>
</li>
<li>
<p>Big data analysis (Hive/Spark reports), to reduce report output time and costs of computing clusters.</p>
</li>
<li>
<p>Data lakehouse, to reduce request costs and the catalog latency.</p>
</li>
<li>
<p>AI and training acceleration, to reduce the costs of using AI clusters and provide more comprehensive capability support.</p>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="JindoCache" src="/zh/assets/images/ali-jindo-bd3a18eb06b9708e37247c130d1be128.png" width="1000" height="615" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="kubedl">KubeDL<a href="#kubedl" class="hash-link" aria-label="KubeDL的直接链接" title="KubeDL的直接链接">​</a></h4>
<p>KubeDL is a Kubernetes (ASI)-based AI workload orchestration system for managing the lifecycle of distributed AI workloads, interaction with layer-1 scheduling, failure tolerance and recovery, as well as dataset and runtime acceleration. It supports the stable operation of more than 10,000 AI training tasks on different platforms in the unified resource pool of Alibaba Group every day, including but not limited to tasks related to Taobao, Alimama, and DAMO Academy business domains. You can download the <a href="https://github.com/kubedl-io/kubedl" target="_blank" rel="noopener noreferrer">open source edition of KubeDL</a> from GitHub.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="overall-project-architecture">Overall Project Architecture<a href="#overall-project-architecture" class="hash-link" aria-label="Overall Project Architecture的直接链接" title="Overall Project Architecture的直接链接">​</a></h4>
<p><img decoding="async" loading="lazy" alt="architecture" src="/zh/assets/images/ali-architecture-ab33bffb6d5c85a262ad64e5774981be.png" width="1000" height="1071" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="32-benefits-of-jindocache-based-fluid">3.2 Benefits of JindoCache-based Fluid<a href="#32-benefits-of-jindocache-based-fluid" class="hash-link" aria-label="3.2 Benefits of JindoCache-based Fluid的直接链接" title="3.2 Benefits of JindoCache-based Fluid的直接链接">​</a></h3>
<ol>
<li>
<p>Fluid can orchestrate datasets in Kubernetes clusters to co-deploy data and computing, and provide PVC-based APIs for seamlessly integrating Kubernetes applications. JindoRuntime can accelerate data access and caching in OSS. POSIX-based APIs of FUSE allow you to easily access large numbers of files in OSS the way you access local disks. Deep learning training tools such as PyTorch can read training data through POSIX-based APIs.</p>
</li>
<li>
<p>Fluid provides distributed metadata and data caches.</p>
</li>
<li>
<p>Fluid supports metadata preloading to prevent a large number of metadata operations on training files in OSS, and supports data preloading to prevent contention for data access due to data pulling during training.</p>
</li>
<li>
<p>Data affinity scheduling of Fluid is called by using KubeDL. You do not need to know the locations of nodes where caches are stored or nodes that may be migrated at any time in scaling scenarios. Fluid schedules tasks with data dependencies in combination with cache nodes to short-circuit read operations and maximize performance.</p>
</li>
<li>
<p>JindoCache provides a variety of distributed caches. You can select appropriate cache policies based on your business requirements. For example, the cache-aside (lazy loading) policy is selected for cache read operations: When an application needs to read data, it first checks the cache to determine whether the data is available. If the data is available (the cache is hit), the cached data is returned. If the data is unavailable (the cache is missed), the data is queried from the underlying storage, added to the cache, and then returned to the caller. In addition, the write-through policy is selected for cache write operations. Files written by an application to the underlying file system are also written to the cache system. This way, this part of data can be directly read from the cache system next time, thus greatly improving read efficiency.</p>
</li>
<li>
<p>Fluid supports self-healing of FUSE mount targets. It can automatically check and recover disconnection of FUSE mount targets caused by out-of-memory (OOM) and other exceptions. This prevents data access exceptions and ensures the stable operation of online business on AI platforms.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="33-practice">3.3 Practice<a href="#33-practice" class="hash-link" aria-label="3.3 Practice的直接链接" title="3.3 Practice的直接链接">​</a></h3>
<p>In practice within Alibaba Group, end-to-end data I/O acceleration is provided for AI platforms based on job orchestration capabilities of KubeDL and caching capabilities of JindoRuntime-based Fluid in combination with idle local resources such as memory and high-performance disks in the huge heterogeneous computing resource pool of Alibaba Group.</p>
<ol>
<li>The huge unified heterogeneous resource pool of Alibaba Group provides resource sales levels with differentiated service level objectives (SLOs) and contains a variety of resources at different levels, including highly guaranteed resources, spot instance resources, periodically offline resources, and normal offline resources. Models, SSDs, and high-performance network interface cards (NICs) of different generations are used with multi-level cache media of JindoCache to make full use of idle resources in the unified resource pool.</li>
<li>Based on the composition of JindoCache clusters, highly guaranteed computing resources are used to run metadata services, and elastic offline resources are used to run I/O bound cache node services. This fully incorporates the scheduling characteristics of the resource pool of Alibaba Group and minimizes user costs.</li>
<li>KubeDL provides distributed training task management, and Fluid provides dataset management. They support automatic reuse of the same data sources of different users across jobs and even automatic reuse of the same data sources of different platforms in the unified resource pool. Based on the reference count of jobs, KubeDL can automatically recycle idle datasets to reduce user costs.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="34-experience">3.4 Experience<a href="#34-experience" class="hash-link" aria-label="3.4 Experience的直接链接" title="3.4 Experience的直接链接">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="experience-is-summarized-from-the-practice-in-the-following-five-aspects">Experience is summarized from the practice in the following five aspects:<a href="#experience-is-summarized-from-the-practice-in-the-following-five-aspects" class="hash-link" aria-label="Experience is summarized from the practice in the following five aspects:的直接链接" title="Experience is summarized from the practice in the following five aspects:的直接链接">​</a></h4>
<ol>
<li>
<p>Select appropriate cache nodes: JindoRuntime can achieve higher local data access performance. In production, nodes with poor disk and network I/O performance are not suitable for caching data. Therefore, select nodes with large disk sizes and high network I/O performance. Fluid supports scheduling of datasets, in other words, scheduling of cache nodes. It schedules cache nodes based on the node affinity of datasets so that the cache nodes can provide efficient cache services.</p>
</li>
<li>
<p>Configure the cache capacity and path: You can specify the <code>mounts</code> field of datasets and tieredstore of JindoRuntime to set the mount directory of data. To prevent excessive data from being cached, you can specify tieredstore of JindoRuntime to limit the maximum cache capacity and watermark. Data that exceeds the watermark is automatically discarded. You can also specify tieredstore to set the cache path and storage media, such as SSD, memory, or HDD, to meet the needs of various scenarios. In multi-node scenarios, the replacement feature of datasets allow you to deploy multiple datasets in one cluster.</p>
</li>
<li>
<p>Set cache security policies: When you create a dataset in Fluid, you may need to configure sensitive information in <code>mounts</code>, such as the AccessKey ID and AccessKey secret of your OSS account. To ensure security, Fluid allows you to use secrets to configure the sensitive information. You can create a secret and specify the name of the secret in the <code>EncryptOptions</code> field of the dataset to bind the sensitive information.</p>
</li>
<li>
<p>Data preloading: After you create a dataset and JindoRuntime, all files in the data directory are downloaded when you access the mounted data for the first time. This causes a waste of space and network resources if the data directory contains data that is not needed. To prevent this problem, Fluid supports data preloading and metadata caching. You can create a dataload to read the path of data to be preloaded. This way, data can be dynamically injected. Dataloads can cache metadata and block access to non-preloaded data, thus greatly improving the efficiency of data access.</p>
</li>
<li>
<p>Enable self-healing of FUSE mount targets in Fluid: During the operation of online business, FUSE processes may crash and restart due to insufficient memory resources. Consequently, FUSE mount targets in the business container are disconnected, and data access exceptions occur, affecting the availability of online business. You can enable self-healing of FUSE mount targets in Fluid. This way, Fluid automatically detects and fixes disconnected mount targets to continuously ensure the stable operation of online business.</p>
</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="35-results">3.5 Results<a href="#35-results" class="hash-link" aria-label="3.5 Results的直接链接" title="3.5 Results的直接链接">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="read-sample-acceleration">Read Sample Acceleration<a href="#read-sample-acceleration" class="hash-link" aria-label="Read Sample Acceleration的直接链接" title="Read Sample Acceleration的直接链接">​</a></h4>
<p>The effect of Fluid is verified in an end-to-end manner based on real user jobs in the production environment.</p>
<p><strong>Task</strong>: LLaMA 13B pre-training task</p>
<p><strong>Test environment:</strong></p>
<p>Cluster and model: high-performance A800 server cluster equipped with remote direct memory access (RDMA) NICs and NVMe disks</p>
<p><strong>Default specifications of Fluid :</strong> 24 × 32Gi cache workers and NVMe disks, which are more cost-effective than memory</p>
<p><strong>Conclusion:</strong></p>
<p><strong>LLaMA 13B Pre-trained Model</strong></p>
<table><thead><tr><th>I/O access mode</th><th>GPU Util</th><th>SM Util</th><th>TFLOPs (log)</th><th>TFLOPs (amperf)</th></tr></thead><tbody><tr><td>Direct connection</td><td>100%</td><td>~60%</td><td>~135</td><td>Up to 60 (avg: 10m)</td></tr><tr><td>Fluid with JindoRuntime</td><td>100%</td><td>Up to 80% (increased by 33%)</td><td>Up to 160 (increased by 18%)</td><td>Up to 72 (avg: 10m) (increased by 20%)</td></tr></tbody></table>
<p><strong>Monitoring Data: Direct Connection without Caching</strong></p>
<p><img decoding="async" loading="lazy" alt="w/o-cache-1" src="/zh/assets/images/ali-wo-cache-1-d3671b03ad6ca54ced00a43fb14dc15f.png" width="1000" height="357" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="w/o-cache-2" src="/zh/assets/images/ali-wo-cache-2-9357516db7ec35961405f257f200ce6c.png" width="1000" height="370" class="img_ev3q"></p>
<p><img decoding="async" loading="lazy" alt="w/o-cache-3" src="/zh/assets/images/ali-wo-cache-3-10b74b5380b758fd38d168855e13bd83.png" width="1000" height="364" class="img_ev3q"></p>
<p><strong>Monitoring Data: Caching Enabled</strong></p>
<p><img decoding="async" loading="lazy" alt="with-cache-1" src="/zh/assets/images/ali-with-cache-1-b62e99da573ac9ea971f266f7b68f8d4.png" width="989" height="377" class="img_ev3q"></p>
<p>The overall average GPU utilization is also close to 100%, and the loads of GPUs are uniform and are all close to 100%.</p>
<p><img decoding="async" loading="lazy" alt="with-cache-2" src="/zh/assets/images/ali-with-cache-2-460b92a8829207a720c1d6144450ff3f.png" width="1000" height="392" class="img_ev3q"></p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="checkpoint-acceleration">Checkpoint Acceleration<a href="#checkpoint-acceleration" class="hash-link" aria-label="Checkpoint Acceleration的直接链接" title="Checkpoint Acceleration的直接链接">​</a></h4>
<p><strong>Training and Offline Inference Scenarios</strong>
A distributed training task loads a checkpoint model file to continue training each time it is restarted. The model size ranges from tens of GB to hundreds of MB. In addition, a large number of offline inference tasks occupy many spot instance resources in the unified resource pool. Resources of an inference task can be preempted at any time, and the task will reload the model for offline inference after a failover. Therefore, a large number of jobs load the same checkpoint file after restart.</p>
<p>Distributed cache acceleration of Fluid converts multiple remote read operations into a single local read operation. This greatly accelerates job failovers and prevents bandwidth costs caused by multiple repeated read operations. In a typical large model scenario, the size of the model file is approximately 20 GB based on the 7B parameter size with FP16 precision. Fluid reduces the model loading time from 10 minutes to approximately 30 seconds.</p>
<p><img decoding="async" loading="lazy" alt="Inference" src="/zh/assets/images/ali-inference-4c22062a009e5348bc46c7dc49f44f41.png" width="1000" height="243" class="img_ev3q"></p>
<p><strong>Spot Scenarios of Training (write-through)</strong>
In spot scenarios of distributed training, if resources of a synchronous training task are preempted, it is usually restarted globally through a failover to continue training. KubeDL cooperates with layer-1 scheduling to instruct, through interactive preemption, the rank 0 node of the training task to record an on-demand checkpoint to save the latest training progress. After the restart, the task can reload the latest checkpoint to continue training as soon as possible. This leverages low costs of spot instance resources and minimizes the costs of training interruption.</p>
<p>The write-through feature in the latest version of JindoRuntime of Fluid allows a restarted task to immediately load the latest model file from the local cache cluster, instead of passively loading the latest model file from the remote storage. The end-to-end failover time is reduced from 10 minutes to 2 minutes on average. This reduces the loss of idle computing power by 80%.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-summary-and-outlook">5. Summary and Outlook<a href="#5-summary-and-outlook" class="hash-link" aria-label="5. Summary and Outlook的直接链接" title="5. Summary and Outlook的直接链接">​</a></h2>
<p>JindoRuntime-based Fluid plays an important role in large-scale machine learning model training for Alibaba Group. In read sample acceleration, it greatly increases the system throughput and achieves more balanced load utilization between GPUs. In addition, the abstraction layer of JindoRuntime shields the differences between JindoCache versions, thus supporting seamless upgrades. In checkpoint acceleration, end-to-end model loading is significantly accelerated, thus significantly improving performance at low costs.</p>
<p>In the future, Fluid is to be applied in more scenarios and provide extended features based on existing ones:</p>
<ol>
<li>
<p>Automatically recycles idle datasets based on reference counts, to automatically manage idle datasets.</p>
</li>
<li>
<p>Intelligently preloads data: automatically preloads data based on the data access mode of tasks, preloads or evicts data by directory priority, and supports splitting of a preload task by directory for parallel preloading.</p>
</li>
<li>
<p>Adopts the RDMA technology to increase the throughput of workers in clusters, to fully utilize high-performance network infrastructure of clusters.</p>
</li>
</ol>
<p>Based on cache acceleration capabilities and multi-JindoCache orchestration capabilities of Fluid, the operation mode and integration of upper-layer systems are to be optimized, to improve hardware and software collaboration and further enhance performance and support new hardware.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="reference">Reference<a href="#reference" class="hash-link" aria-label="Reference的直接链接" title="Reference的直接链接">​</a></h2>
<p>[1] Fluid: <a href="https://github.com/fluid-cloudnative/fluid" target="_blank" rel="noopener noreferrer">https://github.com/fluid-cloudnative/fluid</a></p>
<p>[2] JindoCache: <a href="https://github.com/aliyun/alibabacloud-jindodata/blob/master/docs/user/6.x/6.2.0/jindo_fluid/jindo_fluid_overview.md" target="_blank" rel="noopener noreferrer">https://github.com/aliyun/alibabacloud-jindodata/blob/master/docs/user/6.x/6.2.0/jindo_fluid/jindo_fluid_overview.md</a></p></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/fluid-cloudnative/fluid-cloudnative.github.io/tree/master/versioned_docs/version-v0.9/case-study/alibaba-case-study.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>编辑此页</a></div><div class="col lastUpdated_vwxv"><span class="theme-last-updated">最后<!-- -->由 <b>chenqiming</b> <!-- -->于 <b><time datetime="2024-04-25T10:46:23.000Z">2024年4月25日</time></b> <!-- -->更新</span></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件选项卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/zh/docs/v0.9/developer-guide/client-usage/access-via-REST"><div class="pagination-nav__sublabel">上一  页</div><div class="pagination-nav__label">Access via REST</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/zh/docs/v0.9/case-study/weibo-case-study"><div class="pagination-nav__sublabel">下一页</div><div class="pagination-nav__label">Weibo&#x27;s Case Study</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#1-background-information" class="table-of-contents__link toc-highlight">1. Background Information</a></li><li><a href="#2-chanllenges" class="table-of-contents__link toc-highlight">2. Chanllenges</a></li><li><a href="#3-efficient-cache-scheduling-acceleration-system-for-deep-learning-tasks" class="table-of-contents__link toc-highlight">3. Efficient Cache Scheduling Acceleration System for Deep Learning Tasks</a><ul><li><a href="#31-components-of-the-project-architecture" class="table-of-contents__link toc-highlight">3.1 Components of the Project Architecture</a></li><li><a href="#32-benefits-of-jindocache-based-fluid" class="table-of-contents__link toc-highlight">3.2 Benefits of JindoCache-based Fluid</a></li><li><a href="#33-practice" class="table-of-contents__link toc-highlight">3.3 Practice</a></li><li><a href="#34-experience" class="table-of-contents__link toc-highlight">3.4 Experience</a></li><li><a href="#35-results" class="table-of-contents__link toc-highlight">3.5 Results</a></li></ul></li><li><a href="#5-summary-and-outlook" class="table-of-contents__link toc-highlight">5. Summary and Outlook</a></li><li><a href="#reference" class="table-of-contents__link toc-highlight">Reference</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">文档</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh/docs">教程</a></li></ul></div><div class="col footer__col"><div class="footer__title">社区</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://app.slack.com/client/T08PSQ7BQ/C02ADG209SP" target="_blank" rel="noopener noreferrer" class="footer__link-item">CNCF Slack (#fluid channel)<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a class="footer__link-item" href="/zh/community/meeting_schedule">钉钉群 (群号: 32850151)</a></li><li class="footer__item"><a class="footer__link-item" href="/zh/community/meeting_schedule">周会</a></li><li class="footer__item"><a href="https://github.com/fluid-cloudnative/fluid/blob/master/README.md#community" target="_blank" rel="noopener noreferrer" class="footer__link-item">微信群<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">更多</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/fluid-cloudnative/fluid" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 The Fluid Authors. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage page: https://www.linuxfoundation.org/trademark-usage</div></div></div></footer></div>
</body>
</html>